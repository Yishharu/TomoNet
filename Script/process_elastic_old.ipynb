{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import h5py\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model 0\n",
      "Processing model 1\n",
      "Processing model 2\n",
      "Processing model 3\n",
      "Processing model 4\n",
      "Processing model 5\n",
      "Processing model 6\n",
      "Processing model 7\n",
      "Processing model 8\n",
      "Processing model 9\n",
      "Finished processing all models.\n"
     ]
    }
   ],
   "source": [
    "    '''\n",
    "    Transforming seismogram nc files and harmonics pkl files to a single HDF5 file\n",
    "    '''\n",
    "    path = '../DataSet/10000LatinSphericalHarmonicsElasticBall'\n",
    "    model_name = \"LatinSphericalHarmonicsElasticBall\"\n",
    "    hdf5_file_path = f\"{path}/all_data.h5\"\n",
    "    num_models = 10  # Total number of models\n",
    "\n",
    "    disp_dims = (num_models, 37, 37, 3, 150)\n",
    "    station_coords_cartesian_dims = (num_models, 37, 37, 3)\n",
    "    station_coords_spherical_dims = (num_models, 37, 37, 3)\n",
    "    time_dims = (num_models, 150)\n",
    "    harmonics_dims = (num_models, 1215)\n",
    "\n",
    "    with h5py.File(hdf5_file_path, \"w\") as data_hdf5:\n",
    "        # Preallocate datasets for each variable\n",
    "        disp_data = data_hdf5.create_dataset('disp', disp_dims, dtype='float32')\n",
    "        station_coords_cartesian_data = data_hdf5.create_dataset(\"station_coords_cartesian\", station_coords_cartesian_dims, dtype='float32')\n",
    "        station_coords_spherical_data = data_hdf5.create_dataset(\"station_coords_spherical\", station_coords_spherical_dims, dtype='float32')\n",
    "        time_data = data_hdf5.create_dataset(\"time\", time_dims, dtype='float32')\n",
    "        harmonics_data = data_hdf5.create_dataset(\"harmonics\", harmonics_dims, dtype='float32')\n",
    "        \n",
    "        for model_id in range(0, num_models):\n",
    "            print(f\"Processing model {model_id}\")\n",
    "            \n",
    "            # Define paths for seismogram and harmonics\n",
    "            seis_nc_path = f\"{path}/{model_name}{model_id:0>4d}/seismogram_displacement_SYN.nc\"\n",
    "            harmonics_path = f\"{path}/{model_name}{model_id:0>4d}/Spherical_Harmonics.pkl\"\n",
    "\n",
    "            # Attempt to read files\n",
    "            try:\n",
    "                seismogram_nc = Dataset(seis_nc_path, \"r\")\n",
    "                harmonics_pkl = pickle.load(open(harmonics_path, \"rb\"))\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"File not found: {e.filename}\")\n",
    "                continue\n",
    "\n",
    "            disp_data[model_id, :, :, :, :] = np.array(seismogram_nc['disp'])\n",
    "            station_coords_cartesian_data[model_id, :, :, :] = np.array(seismogram_nc['station_coords_cartesian'])\n",
    "            station_coords_spherical_data[model_id, :, :, :] = np.array(seismogram_nc['station_coords_spherical'])\n",
    "            time_data[model_id, :] = np.array(seismogram_nc['time'])\n",
    "            harmonics_data[model_id, :] = np.array(harmonics_pkl['Value'])\n",
    "            \n",
    "            # Close the NetCDF file\n",
    "            seismogram_nc.close()\n",
    "            \n",
    "        print(\"Finished processing all models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model 0\n",
      "Processing model 1\n",
      "Processing model 2\n",
      "Processing model 3\n",
      "Processing model 4\n",
      "Processing model 5\n",
      "Processing model 6\n",
      "Processing model 7\n",
      "Processing model 8\n",
      "Processing model 9\n",
      "Finished processing all models.\n"
     ]
    }
   ],
   "source": [
    "    '''\n",
    "    Transforming wavefield nc files and harmonics pkl files to a single HDF5 file\n",
    "    '''\n",
    "    path = '../DataSet/10000LatinSphericalHarmonicsElasticBall'\n",
    "    model_name = \"LatinSphericalHarmonicsElasticBall\"\n",
    "    hdf5_file_path = f\"{path}/wf_data.h5\"\n",
    "    num_models = 10  # Total number of models\n",
    "\n",
    "    element_coords_sz_dims = (3648, 2)\n",
    "    disp_coef_dims = (num_models, 15, 3648, 16, 3)\n",
    "    harmonics_dims = (num_models, 1215)\n",
    "\n",
    "    with h5py.File(hdf5_file_path, \"w\") as data_hdf5:\n",
    "        # Preallocate datasets for each variable\n",
    "        dtype = 'float32'\n",
    "        element_coords_sz_data = data_hdf5.create_dataset(\"element_coords_sz\", element_coords_sz_dims, dtype='float64')\n",
    "        disp_coef_data = data_hdf5.create_dataset(\"disp_coef\", disp_coef_dims, dtype=dtype)\n",
    "        harmonics_data = data_hdf5.create_dataset(\"harmonics\", harmonics_dims, dtype=dtype)\n",
    "        \n",
    "        for model_id in range(0, num_models):\n",
    "            print(f\"Processing model {model_id}\")\n",
    "            for snapshot_id in range(0, 15):\n",
    "                # Define paths for seismogram and harmonics\n",
    "                wf_nc_path = f\"{path}/{model_name}{model_id:0>4d}/snapshot_coeff/disp_coef_time{snapshot_id}.nc\"\n",
    "                # Attempt to read files\n",
    "                try:\n",
    "                    wf_nc = Dataset(wf_nc_path, \"r\")\n",
    "                except FileNotFoundError as e:\n",
    "                    print(f\"File not found: {e.filename}\")\n",
    "                    continue\n",
    "                coef = np.array(wf_nc['disp_coef']).transpose(1,0,2)\n",
    "                coords = np.array(wf_nc['element_coords_sz'])\n",
    "                if model_id == 0 and snapshot_id == 0:\n",
    "                    coords0 = coords\n",
    "                    dist = np.sqrt(np.sum(coords0**2, axis=1))\n",
    "                    idx = np.argsort(dist)\n",
    "                    coords0 = coords0[idx]\n",
    "                    coef = coef[idx,:,:]\n",
    "                    element_coords_sz_data[:, :] = coords0\n",
    "                    # if snapshot_id == 10:\n",
    "                    #     colors = plt.cm.viridis(coef[:, 0]*1e5)\n",
    "                    #     plt.figure()\n",
    "                    #     plt.scatter(x=coords0[:,0], y=coords0[:,1], c=colors, cmap='viridis')\n",
    "                    #     plt.savefig(f'wf_{model_id}_{snapshot_id}.png')\n",
    "                else:\n",
    "                    mapping = {tuple(point): index for index, point in enumerate(coords)}\n",
    "                    indices = [mapping[tuple(point)] for point in coords0]\n",
    "                    coords = coords[indices]\n",
    "                    assert (coords == coords0).all()\n",
    "                    coef = coef[indices,:,:]\n",
    "                    # if snapshot_id == 10:\n",
    "                    #     colors = plt.cm.viridis(coef[:, 0]*1e5)\n",
    "                    #     plt.figure()\n",
    "                    #     plt.scatter(x=coords[:,0], y=coords[:,1], c=colors, cmap='viridis')\n",
    "                    #     plt.savefig(f'wf_{model_id}_{snapshot_id}.png')\n",
    "\n",
    "                disp_coef_data[model_id, snapshot_id, :, :, :] = coef\n",
    "\n",
    "                # Close the NetCDF file\n",
    "                wf_nc.close()\n",
    "\n",
    "            harmonics_path = f\"{path}/{model_name}{model_id:0>4d}/Spherical_Harmonics.pkl\"\n",
    "            try:\n",
    "                harmonics_pkl = pickle.load(open(harmonics_path, \"rb\"))\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"File not found: {e.filename}\")\n",
    "                continue\n",
    "            harmonics_data[model_id, :] = np.array(harmonics_pkl['Value'])\n",
    "        print(\"Finished processing all models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model 0\n",
      "Processing model 1\n",
      "Processing model 2\n",
      "Processing model 3\n",
      "Processing model 4\n",
      "Processing model 5\n",
      "Processing model 6\n",
      "Processing model 7\n",
      "Processing model 8\n",
      "Processing model 9\n",
      "Finished processing all models.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    '''\n",
    "    Transforming wavefield nc files and harmonics pkl files to a single HDF5 file\n",
    "    '''\n",
    "    path = '../DataSet/10000LatinSphericalHarmonicsElasticBall'\n",
    "    model_name = \"LatinSphericalHarmonicsElasticBall\"\n",
    "    hdf5_file_path = f\"{path}/wf_slice_data.h5\"\n",
    "    num_models = 10  # Total number osf models\n",
    "\n",
    "    element_coords_cartesian_dims = (16, 3648, 3)\n",
    "    disp_dims = (num_models, 15, 16, 3648, 3) # model_id, snapshot_id, slice_id, element_id, component_id\n",
    "    harmonics_dims = (num_models, 1215)\n",
    "\n",
    "    with h5py.File(hdf5_file_path, \"w\") as data_hdf5:\n",
    "        # Preallocate datasets for each variable\n",
    "        dtype = 'float32'\n",
    "        element_coords_cartesian_data = data_hdf5.create_dataset(\"element_coords_cartesian\", element_coords_cartesian_dims, dtype='float64')\n",
    "        disp_data = data_hdf5.create_dataset(\"disp\", disp_dims, dtype=dtype)\n",
    "        harmonics_data = data_hdf5.create_dataset(\"harmonics\", harmonics_dims, dtype=dtype)\n",
    "        for model_id in range(0, num_models):\n",
    "            print(f\"Processing model {model_id}\")\n",
    "            for snapshot_id in range(0, 15):\n",
    "                for slice_id in range(0, 16):\n",
    "                    # Define paths for seismogram and harmonics\n",
    "                    wf_nc_path = f\"{path}/{model_name}{model_id:0>4d}/snapshot/time{snapshot_id}/disp_slice{slice_id}.nc\"\n",
    "                    # Attempt to read files\n",
    "                    try:\n",
    "                        wf_nc = Dataset(wf_nc_path, \"r\")\n",
    "                    except FileNotFoundError as e:\n",
    "                        print(f\"File not found: {e.filename}\")\n",
    "                        continue\n",
    "                    disp = np.array(wf_nc['disp'])\n",
    "                    coords = np.array(wf_nc['element_coords_cartesian'])\n",
    "                    if model_id == 0 and snapshot_id == 0:\n",
    "                        dist = np.sqrt(np.sum(coords**2, axis=1))\n",
    "                        idx = np.argsort(dist)\n",
    "                        coords = coords[idx]\n",
    "                        element_coords_cartesian_data[slice_id, :, :] = coords\n",
    "                        disp = disp[idx]\n",
    "                    else:\n",
    "                        # print(element_coords_cartesian_data[slice_id, :, :], coords)\n",
    "                        mapping = {tuple(point): index for index, point in enumerate(coords)}\n",
    "                        indices = [mapping[tuple(point)] for point in element_coords_cartesian_data[slice_id, :, :]]\n",
    "                        assert (coords[indices] == element_coords_cartesian_data[slice_id, :, :]).all()\n",
    "                        disp = disp[indices]\n",
    "\n",
    "                    disp_data[model_id, snapshot_id, slice_id, :] = disp\n",
    "\n",
    "                    # Close the NetCDF file\n",
    "                    wf_nc.close()\n",
    "                    # import pdb; pdb.set_trace()\n",
    "                    \n",
    "            harmonics_path = f\"{path}/{model_name}{model_id:0>4d}/Spherical_Harmonics.pkl\"\n",
    "            try:\n",
    "                harmonics_pkl = pickle.load(open(harmonics_path, \"rb\"))\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"File not found: {e.filename}\")\n",
    "                continue\n",
    "            harmonics_data[model_id, :] = np.array(harmonics_pkl['Value'])\n",
    "        print(\"Finished processing all models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcoef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "coef.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3648, 16, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.array(wf_nc['disp_coef']).transpose(1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5da1a19760c645bef876c945e2def5171d007c0fd3f14585be32e516ddabd56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
